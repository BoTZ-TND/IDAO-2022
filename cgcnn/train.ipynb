{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from cgcnn.data import CIFData\n",
    "from cgcnn.data import collate_pool, get_train_val_test_loader\n",
    "from cgcnn.model import CrystalGraphConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.data_options = '../data/dichalcogenides_public/cifs'\n",
    "args.cuda = torch.cuda.is_available()\n",
    "args.print_freq = 10\n",
    "args.task = 'regression'\n",
    "args.batch_size = 32\n",
    "args.train_ratio = None\n",
    "args.workers = 1\n",
    "args.val_ratio = 0.1\n",
    "args.test_ratio = 0.1\n",
    "args.train_size = None\n",
    "args.test_size = None\n",
    "args.val_size = None\n",
    "args.atom_fea_len = 64\n",
    "args.n_conv = 3\n",
    "args.h_fea_len = 128\n",
    "args.n_h = 1\n",
    "args.optim = 'Adam'\n",
    "args.lr = 0.01\n",
    "args.momentum = 0.9\n",
    "args.weight_decay = 0\n",
    "args.resume = '/tmp/out/model'\n",
    "args.start_epoch = 0\n",
    "args.epochs = 10\n",
    "args.lr_milestones = [100]\n",
    "\n",
    "best_mae_error = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(prediction, target):\n",
    "    \"\"\"\n",
    "    Computes the mean absolute error between prediction and target\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    prediction: torch.Tensor (N, 1)\n",
    "    target: torch.Tensor (N, 1)\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs(target - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_eval(prediction, target):\n",
    "    prediction = np.exp(prediction.numpy())\n",
    "    target = target.numpy()\n",
    "    pred_label = np.argmax(prediction, axis=1)\n",
    "    target_label = np.squeeze(target)\n",
    "    if not target_label.shape:\n",
    "        target_label = np.asarray([target_label])\n",
    "    if prediction.shape[1] == 2:\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            target_label, pred_label, average='binary')\n",
    "        auc_score = metrics.roc_auc_score(target_label, prediction[:, 1])\n",
    "        accuracy = metrics.accuracy_score(target_label, pred_label)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return accuracy, precision, recall, fscore, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, k):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every k epochs\"\"\"\n",
    "    assert type(k) is int\n",
    "    lr = args.lr * (0.1 ** (epoch // k))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, normalizer):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    if args.task == 'regression':\n",
    "        mae_errors = AverageMeter()\n",
    "    else:\n",
    "        accuracies = AverageMeter()\n",
    "        precisions = AverageMeter()\n",
    "        recalls = AverageMeter()\n",
    "        fscores = AverageMeter()\n",
    "        auc_scores = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target, _) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if args.cuda:\n",
    "            input_var = (Variable(input[0].cuda(non_blocking=True)),\n",
    "                         Variable(input[1].cuda(non_blocking=True)),\n",
    "                         input[2].cuda(non_blocking=True),\n",
    "                         [crys_idx.cuda(non_blocking=True) for crys_idx in input[3]])\n",
    "        else:\n",
    "            input_var = (Variable(input[0]),\n",
    "                         Variable(input[1]),\n",
    "                         input[2],\n",
    "                         input[3])\n",
    "        # normalize target\n",
    "        if args.task == 'regression':\n",
    "            target_normed = normalizer.norm(target)\n",
    "        else:\n",
    "            target_normed = target.view(-1).long()\n",
    "        if args.cuda:\n",
    "            target_var = Variable(target_normed.cuda(non_blocking=True))\n",
    "        else:\n",
    "            target_var = Variable(target_normed)\n",
    "\n",
    "        # compute output\n",
    "        output = model(*input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        if args.task == 'regression':\n",
    "            mae_error = mae(normalizer.denorm(output.data.cpu()), target)\n",
    "            losses.update(loss.data.cpu(), target.size(0))\n",
    "            mae_errors.update(mae_error, target.size(0))\n",
    "        else:\n",
    "            accuracy, precision, recall, fscore, auc_score = \\\n",
    "                class_eval(output.data.cpu(), target)\n",
    "            losses.update(loss.data.cpu().item(), target.size(0))\n",
    "            accuracies.update(accuracy, target.size(0))\n",
    "            precisions.update(precision, target.size(0))\n",
    "            recalls.update(recall, target.size(0))\n",
    "            fscores.update(fscore, target.size(0))\n",
    "            auc_scores.update(auc_score, target.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            if args.task == 'regression':\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'MAE {mae_errors.val:.3f} ({mae_errors.avg:.3f})'.format(\n",
    "                    epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                    data_time=data_time, loss=losses, mae_errors=mae_errors)\n",
    "                )\n",
    "            else:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Accu {accu.val:.3f} ({accu.avg:.3f})\\t'\n",
    "                      'Precision {prec.val:.3f} ({prec.avg:.3f})\\t'\n",
    "                      'Recall {recall.val:.3f} ({recall.avg:.3f})\\t'\n",
    "                      'F1 {f1.val:.3f} ({f1.avg:.3f})\\t'\n",
    "                      'AUC {auc.val:.3f} ({auc.avg:.3f})'.format(\n",
    "                    epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                    data_time=data_time, loss=losses, accu=accuracies,\n",
    "                    prec=precisions, recall=recalls, f1=fscores,\n",
    "                    auc=auc_scores)\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, normalizer, test=False):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    if args.task == 'regression':\n",
    "        mae_errors = AverageMeter()\n",
    "    else:\n",
    "        accuracies = AverageMeter()\n",
    "        precisions = AverageMeter()\n",
    "        recalls = AverageMeter()\n",
    "        fscores = AverageMeter()\n",
    "        auc_scores = AverageMeter()\n",
    "    if test:\n",
    "        test_targets = []\n",
    "        test_preds = []\n",
    "        test_cif_ids = []\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target, batch_cif_ids) in enumerate(val_loader):\n",
    "        if args.cuda:\n",
    "            with torch.no_grad():\n",
    "                input_var = (Variable(input[0].cuda(non_blocking=True)),\n",
    "                             Variable(input[1].cuda(non_blocking=True)),\n",
    "                             input[2].cuda(non_blocking=True),\n",
    "                             [crys_idx.cuda(non_blocking=True) for crys_idx in input[3]])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                input_var = (Variable(input[0]),\n",
    "                             Variable(input[1]),\n",
    "                             input[2],\n",
    "                             input[3])\n",
    "        if args.task == 'regression':\n",
    "            target_normed = normalizer.norm(target)\n",
    "        else:\n",
    "            target_normed = target.view(-1).long()\n",
    "        if args.cuda:\n",
    "            with torch.no_grad():\n",
    "                target_var = Variable(target_normed.cuda(non_blocking=True))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                target_var = Variable(target_normed)\n",
    "\n",
    "        # compute output\n",
    "        output = model(*input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        if args.task == 'regression':\n",
    "            mae_error = mae(normalizer.denorm(output.data.cpu()), target)\n",
    "            losses.update(loss.data.cpu().item(), target.size(0))\n",
    "            mae_errors.update(mae_error, target.size(0))\n",
    "            if test:\n",
    "                test_pred = normalizer.denorm(output.data.cpu())\n",
    "                test_target = target\n",
    "                test_preds += test_pred.view(-1).tolist()\n",
    "                test_targets += test_target.view(-1).tolist()\n",
    "                test_cif_ids += batch_cif_ids\n",
    "        else:\n",
    "            accuracy, precision, recall, fscore, auc_score = \\\n",
    "                class_eval(output.data.cpu(), target)\n",
    "            losses.update(loss.data.cpu().item(), target.size(0))\n",
    "            accuracies.update(accuracy, target.size(0))\n",
    "            precisions.update(precision, target.size(0))\n",
    "            recalls.update(recall, target.size(0))\n",
    "            fscores.update(fscore, target.size(0))\n",
    "            auc_scores.update(auc_score, target.size(0))\n",
    "            if test:\n",
    "                test_pred = torch.exp(output.data.cpu())\n",
    "                test_target = target\n",
    "                assert test_pred.shape[1] == 2\n",
    "                test_preds += test_pred[:, 1].tolist()\n",
    "                test_targets += test_target.view(-1).tolist()\n",
    "                test_cif_ids += batch_cif_ids\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            if args.task == 'regression':\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'MAE {mae_errors.val:.3f} ({mae_errors.avg:.3f})'.format(\n",
    "                    i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    mae_errors=mae_errors))\n",
    "            else:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Accu {accu.val:.3f} ({accu.avg:.3f})\\t'\n",
    "                      'Precision {prec.val:.3f} ({prec.avg:.3f})\\t'\n",
    "                      'Recall {recall.val:.3f} ({recall.avg:.3f})\\t'\n",
    "                      'F1 {f1.val:.3f} ({f1.avg:.3f})\\t'\n",
    "                      'AUC {auc.val:.3f} ({auc.avg:.3f})'.format(\n",
    "                    i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    accu=accuracies, prec=precisions, recall=recalls,\n",
    "                    f1=fscores, auc=auc_scores))\n",
    "\n",
    "    if test:\n",
    "        star_label = '**'\n",
    "        import csv\n",
    "        with open('test_results.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for cif_id, target, pred in zip(test_cif_ids, test_targets,\n",
    "                                            test_preds):\n",
    "                writer.writerow((cif_id, target, pred))\n",
    "    else:\n",
    "        star_label = '*'\n",
    "    if args.task == 'regression':\n",
    "        print(' {star} MAE {mae_errors.avg:.3f}'.format(star=star_label,\n",
    "                                                        mae_errors=mae_errors))\n",
    "        return mae_errors.avg\n",
    "    else:\n",
    "        print(' {star} AUC {auc.avg:.3f}'.format(star=star_label,\n",
    "                                                 auc=auc_scores))\n",
    "        return auc_scores.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = torch.mean(tensor)\n",
    "        self.std = torch.std(tensor)\n",
    "\n",
    "    def norm(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "    def denorm(self, normed_tensor):\n",
    "        return normed_tensor * self.std + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global args, best_mae_error\n",
    "\n",
    "    # load data\n",
    "    dataset = CIFData(args.data_options)\n",
    "    collate_fn = collate_pool\n",
    "    train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=args.batch_size,\n",
    "        train_ratio=args.train_ratio,\n",
    "        num_workers=args.workers,\n",
    "        val_ratio=args.val_ratio,\n",
    "        test_ratio=args.test_ratio,\n",
    "        pin_memory=args.cuda,\n",
    "        train_size=args.train_size,\n",
    "        val_size=args.val_size,\n",
    "        test_size=args.test_size,\n",
    "        return_test=True)\n",
    "\n",
    "    # obtain target value normalizer\n",
    "    if args.task == 'classification':\n",
    "        normalizer = Normalizer(torch.zeros(2))\n",
    "        normalizer.load_state_dict({'mean': 0., 'std': 1.})\n",
    "    else:\n",
    "        if len(dataset) < 500:\n",
    "            warnings.warn('Dataset has less than 500 data points. '\n",
    "                          'Lower accuracy is expected. ')\n",
    "            sample_data_list = [dataset[i] for i in range(len(dataset))]\n",
    "        else:\n",
    "            sample_data_list = [dataset[i] for i in\n",
    "                                sample(range(len(dataset)), 500)]\n",
    "        _, sample_target, _ = collate_pool(sample_data_list)\n",
    "        normalizer = Normalizer(sample_target)\n",
    "\n",
    "    # build model\n",
    "    structures, _, _ = dataset[0]\n",
    "    orig_atom_fea_len = structures[0].shape[-1]\n",
    "    nbr_fea_len = structures[1].shape[-1]\n",
    "    model = CrystalGraphConvNet(orig_atom_fea_len, nbr_fea_len,\n",
    "                                atom_fea_len=args.atom_fea_len,\n",
    "                                n_conv=args.n_conv,\n",
    "                                h_fea_len=args.h_fea_len,\n",
    "                                n_h=args.n_h,\n",
    "                                classification=True if args.task ==\n",
    "                                                       'classification' else False)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    # define loss func and optimizer\n",
    "    if args.task == 'classification':\n",
    "        criterion = nn.NLLLoss()\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), args.lr,\n",
    "                              momentum=args.momentum,\n",
    "                              weight_decay=args.weight_decay)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), args.lr,\n",
    "                               weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        raise NameError('Only SGD or Adam is allowed as --optim')\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_mae_error = checkpoint['best_mae_error']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            normalizer.load_state_dict(checkpoint['normalizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    scheduler = MultiStepLR(optimizer, milestones=args.lr_milestones,\n",
    "                            gamma=0.1)\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch, normalizer)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        mae_error = validate(val_loader, model, criterion, normalizer)\n",
    "\n",
    "        if mae_error != mae_error:\n",
    "            print('Exit due to NaN')\n",
    "            sys.exit(1)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # remember the best mae_eror and save checkpoint\n",
    "        if args.task == 'regression':\n",
    "            is_best = mae_error < best_mae_error\n",
    "            best_mae_error = min(mae_error, best_mae_error)\n",
    "        else:\n",
    "            is_best = mae_error > best_mae_error\n",
    "            best_mae_error = max(mae_error, best_mae_error)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_mae_error': best_mae_error,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'normalizer': normalizer.state_dict(),\n",
    "            'args': vars(args)\n",
    "        }, is_best)\n",
    "\n",
    "    # test best model\n",
    "    print('---------Evaluate Model on Test Set---------------')\n",
    "    best_checkpoint = torch.load('model_best.pth.tar')\n",
    "    model.load_state_dict(best_checkpoint['state_dict'])\n",
    "    validate(test_loader, model, criterion, normalizer, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] train_ratio is None, using 1 - val_ratio - test_ratio = 0.8 as training data.\n",
      "=> no checkpoint found at '/tmp/out/model'\n",
      "Epoch: [0][0/75]\tTime 6.361 (6.361)\tData 6.224 (6.224)\tLoss 1.1031 (1.1031)\tMAE 0.462 (0.462)\n",
      "Epoch: [0][10/75]\tTime 3.588 (3.990)\tData 3.519 (3.864)\tLoss 0.7628 (3.5392)\tMAE 0.387 (0.759)\n",
      "Epoch: [0][20/75]\tTime 3.651 (3.878)\tData 3.600 (3.756)\tLoss 0.7857 (2.2979)\tMAE 0.384 (0.603)\n",
      "Epoch: [0][30/75]\tTime 3.996 (3.852)\tData 3.795 (3.726)\tLoss 0.8864 (1.8349)\tMAE 0.429 (0.544)\n",
      "Epoch: [0][40/75]\tTime 3.787 (3.846)\tData 3.736 (3.723)\tLoss 0.8204 (1.5347)\tMAE 0.381 (0.496)\n",
      "Epoch: [0][50/75]\tTime 3.981 (3.850)\tData 3.810 (3.721)\tLoss 0.2640 (1.3090)\tMAE 0.189 (0.446)\n",
      "Epoch: [0][60/75]\tTime 3.938 (3.844)\tData 3.752 (3.710)\tLoss 0.1137 (1.1255)\tMAE 0.139 (0.401)\n",
      "Epoch: [0][70/75]\tTime 3.843 (3.851)\tData 3.676 (3.717)\tLoss 0.1120 (0.9920)\tMAE 0.117 (0.368)\n",
      "Test: [0/10]\tTime 6.460 (6.460)\tLoss 0.8055 (0.8055)\tMAE 0.334 (0.334)\n",
      " * MAE 0.339\n",
      "Epoch: [1][0/75]\tTime 6.374 (6.374)\tData 6.131 (6.131)\tLoss 0.1281 (0.1281)\tMAE 0.159 (0.159)\n",
      "Epoch: [1][10/75]\tTime 3.568 (3.839)\tData 3.498 (3.682)\tLoss 0.1281 (0.1374)\tMAE 0.151 (0.139)\n",
      "Epoch: [1][20/75]\tTime 3.796 (3.737)\tData 3.523 (3.589)\tLoss 0.1121 (0.1476)\tMAE 0.129 (0.151)\n",
      "Epoch: [1][30/75]\tTime 3.837 (3.724)\tData 3.641 (3.580)\tLoss 0.0756 (0.1530)\tMAE 0.112 (0.156)\n",
      "Epoch: [1][40/75]\tTime 3.671 (3.708)\tData 3.602 (3.566)\tLoss 0.3513 (0.1510)\tMAE 0.212 (0.154)\n",
      "Epoch: [1][50/75]\tTime 3.696 (3.697)\tData 3.563 (3.558)\tLoss 0.0852 (0.1409)\tMAE 0.131 (0.149)\n",
      "Epoch: [1][60/75]\tTime 3.568 (3.692)\tData 3.484 (3.555)\tLoss 0.0570 (0.1328)\tMAE 0.107 (0.144)\n",
      "Epoch: [1][70/75]\tTime 3.590 (3.691)\tData 3.538 (3.555)\tLoss 0.0611 (0.1282)\tMAE 0.112 (0.142)\n",
      "Test: [0/10]\tTime 6.135 (6.135)\tLoss 11.7270 (11.7270)\tMAE 1.749 (1.749)\n",
      " * MAE 1.802\n",
      "Epoch: [2][0/75]\tTime 6.395 (6.395)\tData 6.256 (6.256)\tLoss 0.1176 (0.1176)\tMAE 0.140 (0.140)\n",
      "Epoch: [2][10/75]\tTime 3.649 (3.835)\tData 3.485 (3.697)\tLoss 0.1469 (0.1244)\tMAE 0.169 (0.145)\n",
      "Epoch: [2][20/75]\tTime 3.773 (3.721)\tData 3.540 (3.573)\tLoss 0.0612 (0.1126)\tMAE 0.116 (0.138)\n",
      "Epoch: [2][30/75]\tTime 3.521 (3.681)\tData 3.435 (3.539)\tLoss 0.0669 (0.1078)\tMAE 0.102 (0.134)\n",
      "Epoch: [2][40/75]\tTime 3.671 (3.668)\tData 3.474 (3.527)\tLoss 0.0972 (0.1086)\tMAE 0.141 (0.134)\n",
      "Epoch: [2][50/75]\tTime 3.792 (3.661)\tData 3.546 (3.519)\tLoss 0.0442 (0.1085)\tMAE 0.087 (0.135)\n",
      "Epoch: [2][60/75]\tTime 3.532 (3.656)\tData 3.481 (3.517)\tLoss 0.0454 (0.1028)\tMAE 0.084 (0.131)\n",
      "Epoch: [2][70/75]\tTime 3.798 (3.659)\tData 3.600 (3.520)\tLoss 0.0372 (0.1041)\tMAE 0.079 (0.131)\n",
      "Test: [0/10]\tTime 6.272 (6.272)\tLoss 0.2263 (0.2263)\tMAE 0.167 (0.167)\n",
      " * MAE 0.169\n",
      "Epoch: [3][0/75]\tTime 6.389 (6.389)\tData 6.215 (6.215)\tLoss 0.0291 (0.0291)\tMAE 0.079 (0.079)\n",
      "Epoch: [3][10/75]\tTime 3.378 (3.807)\tData 3.309 (3.661)\tLoss 0.0317 (0.0551)\tMAE 0.078 (0.100)\n",
      "Epoch: [3][20/75]\tTime 3.346 (3.705)\tData 3.277 (3.559)\tLoss 0.0752 (0.0545)\tMAE 0.129 (0.098)\n",
      "Epoch: [3][30/75]\tTime 3.389 (3.671)\tData 3.320 (3.528)\tLoss 0.0403 (0.0559)\tMAE 0.077 (0.099)\n",
      "Epoch: [3][40/75]\tTime 3.809 (3.660)\tData 3.566 (3.515)\tLoss 0.0408 (0.0584)\tMAE 0.084 (0.100)\n",
      "Epoch: [3][50/75]\tTime 3.430 (3.649)\tData 3.361 (3.504)\tLoss 0.0439 (0.0588)\tMAE 0.089 (0.100)\n",
      "Epoch: [3][60/75]\tTime 3.501 (3.647)\tData 3.431 (3.503)\tLoss 0.0265 (0.0551)\tMAE 0.069 (0.097)\n",
      "Epoch: [3][70/75]\tTime 3.747 (3.650)\tData 3.590 (3.505)\tLoss 0.0223 (0.0527)\tMAE 0.068 (0.094)\n",
      "Test: [0/10]\tTime 6.493 (6.493)\tLoss 2.9475 (2.9475)\tMAE 0.716 (0.716)\n",
      " * MAE 0.648\n",
      "Epoch: [4][0/75]\tTime 7.094 (7.094)\tData 6.846 (6.846)\tLoss 0.0362 (0.0362)\tMAE 0.079 (0.079)\n",
      "Epoch: [4][10/75]\tTime 3.350 (3.809)\tData 3.282 (3.661)\tLoss 0.0319 (0.0546)\tMAE 0.072 (0.099)\n",
      "Epoch: [4][20/75]\tTime 3.511 (3.676)\tData 3.345 (3.539)\tLoss 0.0462 (0.0609)\tMAE 0.071 (0.103)\n",
      "Epoch: [4][30/75]\tTime 3.417 (3.620)\tData 3.367 (3.490)\tLoss 0.0213 (0.0609)\tMAE 0.059 (0.103)\n",
      "Epoch: [4][40/75]\tTime 3.394 (3.593)\tData 3.308 (3.461)\tLoss 0.0419 (0.0603)\tMAE 0.086 (0.103)\n",
      "Epoch: [4][50/75]\tTime 3.300 (3.585)\tData 3.231 (3.450)\tLoss 0.0168 (0.0544)\tMAE 0.055 (0.096)\n",
      "Epoch: [4][60/75]\tTime 3.444 (3.589)\tData 3.378 (3.456)\tLoss 0.0188 (0.0536)\tMAE 0.058 (0.096)\n",
      "Epoch: [4][70/75]\tTime 4.519 (3.660)\tData 4.368 (3.525)\tLoss 0.0228 (0.0517)\tMAE 0.064 (0.093)\n",
      "Test: [0/10]\tTime 7.379 (7.379)\tLoss 0.0517 (0.0517)\tMAE 0.089 (0.089)\n",
      " * MAE 0.075\n",
      "Epoch: [5][0/75]\tTime 6.232 (6.232)\tData 5.964 (5.964)\tLoss 0.0223 (0.0223)\tMAE 0.071 (0.071)\n",
      "Epoch: [5][10/75]\tTime 3.513 (4.080)\tData 3.356 (3.937)\tLoss 0.0089 (0.0562)\tMAE 0.037 (0.100)\n",
      "Epoch: [5][20/75]\tTime 3.504 (3.793)\tData 3.286 (3.645)\tLoss 0.0497 (0.0570)\tMAE 0.107 (0.102)\n",
      "Epoch: [5][30/75]\tTime 3.385 (3.715)\tData 3.316 (3.565)\tLoss 0.0905 (0.0578)\tMAE 0.119 (0.104)\n",
      "Epoch: [5][40/75]\tTime 3.654 (3.668)\tData 3.422 (3.520)\tLoss 0.0727 (0.0586)\tMAE 0.102 (0.103)\n",
      "Epoch: [5][50/75]\tTime 3.669 (3.651)\tData 3.503 (3.505)\tLoss 0.0276 (0.0546)\tMAE 0.073 (0.099)\n",
      "Epoch: [5][60/75]\tTime 3.489 (3.639)\tData 3.420 (3.496)\tLoss 0.0262 (0.0541)\tMAE 0.051 (0.098)\n",
      "Epoch: [5][70/75]\tTime 3.639 (3.636)\tData 3.494 (3.492)\tLoss 0.0186 (0.0524)\tMAE 0.052 (0.097)\n",
      "Test: [0/10]\tTime 6.133 (6.133)\tLoss 0.0283 (0.0283)\tMAE 0.075 (0.075)\n",
      " * MAE 0.081\n",
      "Epoch: [6][0/75]\tTime 6.470 (6.470)\tData 6.265 (6.265)\tLoss 0.0229 (0.0229)\tMAE 0.065 (0.065)\n",
      "Epoch: [6][10/75]\tTime 3.779 (3.811)\tData 3.512 (3.673)\tLoss 0.2660 (0.0643)\tMAE 0.255 (0.095)\n",
      "Epoch: [6][20/75]\tTime 3.498 (3.693)\tData 3.429 (3.550)\tLoss 0.0250 (0.0640)\tMAE 0.057 (0.098)\n",
      "Epoch: [6][30/75]\tTime 3.387 (3.647)\tData 3.316 (3.505)\tLoss 0.0331 (0.0575)\tMAE 0.077 (0.092)\n",
      "Epoch: [6][40/75]\tTime 3.480 (3.637)\tData 3.411 (3.496)\tLoss 0.0380 (0.0620)\tMAE 0.088 (0.098)\n",
      "Epoch: [6][50/75]\tTime 3.820 (3.652)\tData 3.547 (3.507)\tLoss 0.0600 (0.0645)\tMAE 0.093 (0.101)\n",
      "Epoch: [6][60/75]\tTime 3.901 (3.695)\tData 3.664 (3.549)\tLoss 0.0307 (0.0638)\tMAE 0.084 (0.101)\n",
      "Epoch: [6][70/75]\tTime 3.579 (3.722)\tData 3.510 (3.580)\tLoss 0.0119 (0.0595)\tMAE 0.045 (0.098)\n",
      "Test: [0/10]\tTime 6.798 (6.798)\tLoss 0.0803 (0.0803)\tMAE 0.133 (0.133)\n",
      " * MAE 0.134\n",
      "Epoch: [7][0/75]\tTime 6.220 (6.220)\tData 6.077 (6.077)\tLoss 0.0050 (0.0050)\tMAE 0.029 (0.029)\n",
      "Epoch: [7][10/75]\tTime 3.505 (3.758)\tData 3.275 (3.593)\tLoss 0.0602 (0.0458)\tMAE 0.114 (0.089)\n",
      "Epoch: [7][20/75]\tTime 3.734 (3.641)\tData 3.547 (3.487)\tLoss 0.0245 (0.0377)\tMAE 0.067 (0.080)\n",
      "Epoch: [7][30/75]\tTime 3.664 (3.611)\tData 3.416 (3.463)\tLoss 0.0163 (0.0371)\tMAE 0.055 (0.080)\n",
      "Epoch: [7][40/75]\tTime 3.638 (3.603)\tData 3.401 (3.458)\tLoss 0.0344 (0.0337)\tMAE 0.082 (0.076)\n",
      "Epoch: [7][50/75]\tTime 3.505 (3.578)\tData 3.435 (3.437)\tLoss 0.0221 (0.0338)\tMAE 0.065 (0.075)\n",
      "Epoch: [7][60/75]\tTime 3.637 (3.579)\tData 3.474 (3.441)\tLoss 0.0308 (0.0324)\tMAE 0.086 (0.074)\n",
      "Epoch: [7][70/75]\tTime 3.370 (3.572)\tData 3.300 (3.433)\tLoss 0.0402 (0.0310)\tMAE 0.081 (0.072)\n",
      "Test: [0/10]\tTime 6.064 (6.064)\tLoss 0.0212 (0.0212)\tMAE 0.059 (0.059)\n",
      " * MAE 0.073\n",
      "Epoch: [8][0/75]\tTime 6.535 (6.535)\tData 6.345 (6.345)\tLoss 0.0151 (0.0151)\tMAE 0.056 (0.056)\n",
      "Epoch: [8][10/75]\tTime 4.405 (4.030)\tData 4.182 (3.879)\tLoss 0.0046 (0.0173)\tMAE 0.027 (0.055)\n",
      "Epoch: [8][20/75]\tTime 3.359 (3.848)\tData 3.289 (3.709)\tLoss 0.0263 (0.0207)\tMAE 0.053 (0.058)\n",
      "Epoch: [8][30/75]\tTime 3.277 (3.726)\tData 3.207 (3.584)\tLoss 0.0578 (0.0275)\tMAE 0.100 (0.067)\n",
      "Epoch: [8][40/75]\tTime 3.835 (3.766)\tData 3.749 (3.625)\tLoss 0.0588 (0.0265)\tMAE 0.101 (0.064)\n",
      "Epoch: [8][50/75]\tTime 3.795 (3.843)\tData 3.729 (3.699)\tLoss 0.0502 (0.0312)\tMAE 0.101 (0.071)\n",
      "Epoch: [8][60/75]\tTime 3.631 (3.830)\tData 3.581 (3.689)\tLoss 0.0467 (0.0294)\tMAE 0.099 (0.068)\n",
      "Epoch: [8][70/75]\tTime 3.455 (3.846)\tData 3.384 (3.703)\tLoss 0.0145 (0.0289)\tMAE 0.049 (0.068)\n",
      "Test: [0/10]\tTime 6.776 (6.776)\tLoss 0.6000 (0.6000)\tMAE 0.321 (0.321)\n",
      " * MAE 0.317\n",
      "Epoch: [9][0/75]\tTime 5.946 (5.946)\tData 5.814 (5.814)\tLoss 0.0346 (0.0346)\tMAE 0.084 (0.084)\n",
      "Epoch: [9][10/75]\tTime 3.553 (3.903)\tData 3.484 (3.767)\tLoss 0.0036 (0.0345)\tMAE 0.024 (0.078)\n",
      "Epoch: [9][20/75]\tTime 3.483 (3.849)\tData 3.415 (3.707)\tLoss 0.0275 (0.0300)\tMAE 0.063 (0.072)\n",
      "Epoch: [9][30/75]\tTime 3.731 (3.774)\tData 3.493 (3.628)\tLoss 0.0458 (0.0286)\tMAE 0.102 (0.068)\n",
      "Epoch: [9][40/75]\tTime 3.338 (3.722)\tData 3.269 (3.579)\tLoss 0.0169 (0.0271)\tMAE 0.054 (0.067)\n",
      "Epoch: [9][50/75]\tTime 3.425 (3.706)\tData 3.357 (3.562)\tLoss 0.0047 (0.0266)\tMAE 0.028 (0.066)\n",
      "Epoch: [9][60/75]\tTime 3.795 (3.692)\tData 3.535 (3.545)\tLoss 0.0253 (0.0269)\tMAE 0.073 (0.067)\n",
      "Epoch: [9][70/75]\tTime 3.809 (3.702)\tData 3.544 (3.554)\tLoss 0.0068 (0.0266)\tMAE 0.037 (0.066)\n",
      "Test: [0/10]\tTime 6.273 (6.273)\tLoss 0.1409 (0.1409)\tMAE 0.152 (0.152)\n",
      " * MAE 0.163\n",
      "---------Evaluate Model on Test Set---------------\n",
      "Test: [0/10]\tTime 6.612 (6.612)\tLoss 0.0514 (0.0514)\tMAE 0.079 (0.079)\n",
      " ** MAE 0.068\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aacd9efd2e917f2085b49ad3eecd2bc8a974d0bb8b89bc48afae7fa44e9f517f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
