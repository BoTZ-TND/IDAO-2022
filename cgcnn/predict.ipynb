{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from cgcnn.data import CIFData\n",
    "from cgcnn.data import collate_pool\n",
    "from cgcnn.model import CrystalGraphConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "\n",
    "args.modelpath = './checkpoint.pth.tar'\n",
    "args.cifpath = '../data/dichalcogenides_private/cifs'\n",
    "args.batch_size = 32\n",
    "args.workers = 1\n",
    "args.cuda = True\n",
    "args.print_freq = 10\n",
    "\n",
    "best_mae_error = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_eval(prediction, target):\n",
    "    prediction = np.exp(prediction.numpy())\n",
    "    target = target.numpy()\n",
    "    pred_label = np.argmax(prediction, axis=1)\n",
    "    target_label = np.squeeze(target)\n",
    "    if prediction.shape[1] == 2:\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            target_label, pred_label, average='binary')\n",
    "        auc_score = metrics.roc_auc_score(target_label, prediction[:, 1])\n",
    "        accuracy = metrics.accuracy_score(target_label, pred_label)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return accuracy, precision, recall, fscore, auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mae(prediction, target):\n",
    "    \"\"\"\n",
    "    Computes the mean absolute error between prediction and target\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    prediction: torch.Tensor (N, 1)\n",
    "    target: torch.Tensor (N, 1)\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs(target - prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "    def __init__(self, tensor):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = torch.mean(tensor)\n",
    "        self.std = torch.std(tensor)\n",
    "\n",
    "    def norm(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "    def denorm(self, normed_tensor):\n",
    "        return normed_tensor * self.std + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, normalizer, test=False):\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    if test:\n",
    "        test_targets = []\n",
    "        test_preds = []\n",
    "        test_cif_ids = []\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for (inp, target, batch_cif_ids) in tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "            if args.cuda:\n",
    "                input_var = (Variable(inp[0].cuda(non_blocking=True)),\n",
    "                             Variable(inp[1].cuda(non_blocking=True)),\n",
    "                             inp[2].cuda(non_blocking=True),\n",
    "                             [crys_idx.cuda(non_blocking=True) for crys_idx in inp[3]])\n",
    "            else:\n",
    "                input_var = (Variable(inp[0]),\n",
    "                             Variable(inp[1]),\n",
    "                             inp[2],\n",
    "                             inp[3])\n",
    "        if model_args.task == 'regression':\n",
    "            target_normed = normalizer.norm(target)\n",
    "        else:\n",
    "            target_normed = target.view(-1).long()\n",
    "        with torch.no_grad():\n",
    "            if args.cuda:\n",
    "                target_var = Variable(target_normed.cuda(non_blocking=True))\n",
    "            else:\n",
    "                target_var = Variable(target_normed)\n",
    "\n",
    "        # compute output\n",
    "        output = model(*input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        test_pred = normalizer.denorm(output.data.cpu())\n",
    "        test_target = target\n",
    "        test_preds += test_pred.view(-1).tolist()\n",
    "        test_targets += test_target.view(-1).tolist()\n",
    "        test_cif_ids += batch_cif_ids\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "    \n",
    "    star_label = '**'\n",
    "    import csv\n",
    "    with open('test_results.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for cif_id, target, pred in zip(test_cif_ids, test_targets,\n",
    "                                        test_preds):\n",
    "            writer.writerow((cif_id, target, pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = argparse.Namespace()\n",
    "\n",
    "model_args.atom_fea_len = 64\n",
    "model_args.n_conv = 3\n",
    "model_args.h_fea_len = 128\n",
    "model_args.n_h = 1\n",
    "model_args.task = 'regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global args, model_args, best_mae_error\n",
    "\n",
    "    # load data\n",
    "    dataset = CIFData(args.cifpath)\n",
    "    collate_fn = collate_pool\n",
    "    test_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                             num_workers=args.workers, collate_fn=collate_fn,\n",
    "                             pin_memory=args.cuda)\n",
    "\n",
    "    # build model\n",
    "    structures, _, _ = dataset[0]\n",
    "    orig_atom_fea_len = structures[0].shape[-1]\n",
    "    nbr_fea_len = structures[1].shape[-1]\n",
    "    model = CrystalGraphConvNet(orig_atom_fea_len, nbr_fea_len,\n",
    "                                atom_fea_len=model_args.atom_fea_len,\n",
    "                                n_conv=model_args.n_conv,\n",
    "                                h_fea_len=model_args.h_fea_len,\n",
    "                                n_h=model_args.n_h,\n",
    "                                classification=True if model_args.task ==\n",
    "                                'classification' else False)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    # define loss func and optimizer\n",
    "    if model_args.task == 'classification':\n",
    "        criterion = nn.NLLLoss()\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "    # if args.optim == 'SGD':\n",
    "    #     optimizer = optim.SGD(model.parameters(), args.lr,\n",
    "    #                           momentum=args.momentum,\n",
    "    #                           weight_decay=args.weight_decay)\n",
    "    # elif args.optim == 'Adam':\n",
    "    #     optimizer = optim.Adam(model.parameters(), args.lr,\n",
    "    #                            weight_decay=args.weight_decay)\n",
    "    # else:\n",
    "    #     raise NameError('Only SGD or Adam is allowed as --optim')\n",
    "\n",
    "    normalizer = Normalizer(torch.zeros(3))\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if os.path.isfile(args.modelpath):\n",
    "        print(\"=> loading model '{}'\".format(args.modelpath))\n",
    "        checkpoint = torch.load(args.modelpath,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        normalizer.load_state_dict(checkpoint['normalizer'])\n",
    "        print(\"=> loaded model '{}' (epoch {}, validation {})\"\n",
    "              .format(args.modelpath, checkpoint['epoch'],\n",
    "                      checkpoint['best_mae_error']))\n",
    "    else:\n",
    "        print(\"=> no model found at '{}'\".format(args.modelpath))\n",
    "\n",
    "    validate(test_loader, model, criterion, normalizer, test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading model './checkpoint.pth.tar'\n",
      "=> loaded model './checkpoint.pth.tar' (epoch 10, validation 0.07258342951536179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [05:49<00:00,  3.75s/it]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aacd9efd2e917f2085b49ad3eecd2bc8a974d0bb8b89bc48afae7fa44e9f517f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
