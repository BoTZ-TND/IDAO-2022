{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATCNN Implementation in PyG\n",
    "reference : [github](https://github.com/superlouis/GATGNN/tree/master/gatgnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_v2 import *\n",
    "from gatgnn import *\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as sk_MAE\n",
    "from tabulate import tabulate\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP CODE TO RUN ON GPU\n",
    "gpu_id = 0\n",
    "device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADER/ TARGET NORMALIZATION\n",
    "src_CIF = 'CIF-DATA'\n",
    "random_num = 456\n",
    "random.seed(random_num)\n",
    "dataset = pd.read_csv(\n",
    "    '../data/dichalcogenides_public/targets.csv').sample(frac=1, random_state=random_num)\n",
    "NORMALIZER = DATA_normalizer(dataset['band_gap'].values)\n",
    "RSM = {'radius': 4, 'step': 0.5, 'max_num_nbr': 16}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_properties(crystal_property):\n",
    "    if crystal_property in ['poisson-ratio', 'band-gap', 'absolute-energy', 'fermi-energy', 'formation-energy', 'new-property']:\n",
    "        norm_action = None\n",
    "        classification = None\n",
    "    elif crystal_property == 'is_metal':\n",
    "        norm_action = 'classification-1'\n",
    "        classification = 1\n",
    "    elif crystal_property == 'is_not_metal':\n",
    "        norm_action = 'classification-0'\n",
    "        classification = 1\n",
    "    else:\n",
    "        norm_action = 'log'\n",
    "        classification = None\n",
    "    return norm_action, classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_MAE(tensor1, tensor2):\n",
    "    return torch.mean(torch.abs(tensor1-tensor2))\n",
    "\n",
    "\n",
    "def torch_accuracy(pred_tensor, true_tensor):\n",
    "    _, pred_tensor = torch.max(pred_tensor, dim=1)\n",
    "    correct = (pred_tensor == true_tensor).sum().float()\n",
    "    total = pred_tensor.size(0)\n",
    "    accuracy_ans = correct/total\n",
    "    return accuracy_ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_training(metrics_obj, epoch, estop_val, extra='---'):\n",
    "    header_1, header_2 = 'MSE | e-stop', 'MAE | TIME'\n",
    "    if metrics_obj.c_property in ['is_metal', 'is_not_metal']:\n",
    "        header_1, header_2 = 'Cross_E | e-stop', 'Accuracy | TIME'\n",
    "\n",
    "    train_1, train_2 = metrics_obj.training_loss1[epoch], metrics_obj.training_loss2[epoch]\n",
    "    valid_1, valid_2 = metrics_obj.valid_loss1[epoch], metrics_obj.valid_loss2[epoch]\n",
    "\n",
    "    tab_val = [['TRAINING', f'{train_1:.4f}', f'{train_2:.4f}'], [\n",
    "        'VALIDATION', f'{valid_1:.4f}', f'{valid_2:.4f}'], ['E-STOPPING', f'{estop_val}', f'{extra}']]\n",
    "\n",
    "    output = tabulate(tab_val, headers=[\n",
    "                      f'EPOCH # {epoch}', header_1, header_2], tablefmt='fancy_grid')\n",
    "    print(output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics():\n",
    "    saved_metrics = pickle.load(open(\"MODELS/metrics_.pickle\", \"rb\", -1))\n",
    "    return saved_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOdel Training Early Stopping Function\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False, increment=0.001, save_best=True, classification=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.classification = classification\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "        self.increment = increment\n",
    "        self.flag_value = f' *** '\n",
    "        self.FLAG = None\n",
    "        self.save_best = save_best\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score <= self.best_score + self.increment:\n",
    "            if self.classification == None:\n",
    "                self.increase_measure(val_loss, model, score)\n",
    "            else:\n",
    "                self.decrease_measure(val_loss, model, score)\n",
    "        elif score > self.best_score + self.increment:\n",
    "            if self.classification == None:\n",
    "                self.decrease_measure(val_loss, model, score)\n",
    "            else:\n",
    "                self.increase_measure(val_loss, model, score)\n",
    "\n",
    "    def increase_measure(self, val_loss, model, score):\n",
    "        self.counter += 1\n",
    "        self.flag_value = f'> {self.counter} / {self.patience}'\n",
    "        self.FLAG = True\n",
    "        if self.save_best == False:\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "\n",
    "    def decrease_measure(self, val_loss, model, score):\n",
    "        self.best_score = score\n",
    "        self.save_checkpoint(val_loss, model)\n",
    "        self.counter = 0\n",
    "        self.flag_value = f' *** '\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            pass\n",
    "        torch.save(model.state_dict(), '../tmp/models/crystal-checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "        self.FLAG = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CRYSTAL_DATA = CIF_Dataset(\n",
    "    dataset, root_dir='../data/dichalcogenides_public/cifs/', **RSM)\n",
    "idx_list = list(range(len(dataset)))\n",
    "random.shuffle(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_dataset = pd.read_csv('../data/dichalcogenides_private/targets.csv')\n",
    "private_data = CIF_Dataset(\n",
    "    private_dataset, root_dir='../data/dichalcogenides_private/cifs/', **RSM)\n",
    "test_idx = list(range(len(private_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    idx_list, train_size=0.8, random_state=random_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "norm_action, classification = set_model_properties('band_gap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_set = CIF_Lister(train_idx, CRYSTAL_DATA,\n",
    "                          NORMALIZER, norm_action, df=dataset, src='MEGNET')\n",
    "validation_set = CIF_Lister(val_idx, CRYSTAL_DATA,\n",
    "                            NORMALIZER, norm_action,  df=dataset, src='MEGNET')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = CIF_Lister(test_idx, private_data,\n",
    "                            NORMALIZER, norm_action,  df=private_dataset, src='MEGNET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[191, 92], edge_index=[2, 3056], edge_attr=[3056, 9], y=[1], global_feature=[1, 103], cluster=[191], num_atoms=[1], coords=[191, 3], the_idx=[1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Configs ===\n",
    "n_heads = 4\n",
    "number_neurons = 64\n",
    "number_layers = 3\n",
    "xtra_l = True\n",
    "global_att = 'composit'  # ['composit', 'cluster']\n",
    "attention_technique = 'learnable'  # ['fixed', 'random', 'learnable']\n",
    "concat_comp = True\n",
    "data_src = 'MEGNET'  # ['CGCNN','MEGNET','NEW']\n",
    "\n",
    "# ====================\n",
    "# === Model Training Configs ===\n",
    "learning_rate = 5e-3\n",
    "milestones = [150, 250]\n",
    "stop_patience = 150\n",
    "crystal_property = 'band_gap'\n",
    "num_epochs = 200\n",
    "train_param = {'batch_size': 32, 'shuffle': True}\n",
    "valid_param = {'batch_size': 32, 'shuffle': True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NEURAL-NETWORK\n",
    "the_network = GATGNN(n_heads, classification, neurons=number_neurons, nl=number_layers, xtra_layers=xtra_l, global_attention=global_att,\n",
    "                     unpooling_technique=attention_technique, concat_comp=concat_comp, edge_format=data_src)\n",
    "net = the_network.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS & OPTMIZER & SCHEDULER\n",
    "if classification == 1:\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    funct = torch_accuracy\n",
    "else:\n",
    "    criterion = nn.SmoothL1Loss().cuda()\n",
    "    funct = torch_MAE\n",
    "\n",
    "optimizer = optim.AdamW(net.parameters(), lr=learning_rate, weight_decay=1e-1)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.3)\n",
    "\n",
    "# EARLY-STOPPING INITIALIZATION\n",
    "early_stopping = EarlyStopping(patience=stop_patience, increment=1e-6,\n",
    "                               verbose=True, save_best=True, classification=classification)\n",
    "\n",
    "# METRICS-OBJECT INITIALIZATION\n",
    "metrics = METRICS(crystal_property, num_epochs, criterion, funct, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'> TRAINING MODEL ...')\n",
    "train_loader = torch_DataLoader(dataset=training_set,   **train_param)\n",
    "valid_loader = torch_DataLoader(dataset=validation_set, **valid_param)\n",
    "for epoch in range(num_epochs):\n",
    "    # TRAINING-STAGE\n",
    "    net.train()\n",
    "    start_time = time.time()\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        predictions = net(data)\n",
    "        train_label = metrics.set_label('training', data)\n",
    "        loss = metrics('training', predictions, train_label, 1)\n",
    "        _ = metrics('training', predictions, train_label, 2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics.training_counter += 1\n",
    "    metrics.reset_parameters('training', epoch)\n",
    "    # VALIDATION-PHASE\n",
    "    net.eval()\n",
    "    for data in valid_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = net(data)\n",
    "        valid_label = metrics.set_label('validation', data)\n",
    "        _ = metrics('validation', predictions, valid_label, 1)\n",
    "        _ = metrics('validation', predictions, valid_label, 2)\n",
    "\n",
    "        metrics.valid_counter += 1\n",
    "\n",
    "    metrics.reset_parameters('validation', epoch)\n",
    "    scheduler.step()\n",
    "    end_time = time.time()\n",
    "    e_time = end_time-start_time\n",
    "    metrics.save_time(e_time)\n",
    "\n",
    "    # EARLY-STOPPING\n",
    "    early_stopping(metrics.valid_loss2[epoch], net)\n",
    "    flag_value = early_stopping.flag_value + \\\n",
    "        '_'*(22-len(early_stopping.flag_value))\n",
    "    if early_stopping.FLAG == True:\n",
    "        estop_val = flag_value\n",
    "    else:\n",
    "        estop_val = '@best: saving model...'\n",
    "        best_epoch = epoch+1\n",
    "    output_training(metrics, epoch, estop_val, f'{e_time:.1f} sec.')\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"> Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load & Predict for private dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATGNN(\n",
       "  (embed_n): Linear(in_features=92, out_features=64, bias=True)\n",
       "  (embed_e): Linear(in_features=9, out_features=64, bias=True)\n",
       "  (embed_comp): Linear(in_features=103, out_features=64, bias=True)\n",
       "  (node_att): ModuleList(\n",
       "    (0): GAT_Crystal()\n",
       "    (1): GAT_Crystal()\n",
       "    (2): GAT_Crystal()\n",
       "  )\n",
       "  (batch_norm): ModuleList(\n",
       "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (cluster_att): CLUSTER_Attention(\n",
       "    (learn_unpool): Linear(in_features=131, out_features=3, bias=True)\n",
       "    (layer_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (atten_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (comp_atten): COMPOSITION_Attention(\n",
       "    (node_layer1): Linear(in_features=167, out_features=32, bias=True)\n",
       "    (atten_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_model = GATGNN(n_heads, classification, neurons=number_neurons, nl=number_layers, xtra_layers=xtra_l, global_attention=global_att,\n",
    " unpooling_technique=attention_technique, concat_comp=concat_comp, edge_format=data_src)\n",
    "pred_model.load_state_dict(torch.load('../tmp/models/crystal-checkpoint.pt'))\n",
    "pred_model = pred_model.to(device)\n",
    "pred_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_param = {'batch_size': 32, 'shuffle': True}\n",
    "test_loader = torch_DataLoader(dataset=test_set, **test_param)\n",
    "pred_arr = []\n",
    "for data in test_loader:\n",
    "    data = data.to(device)\n",
    "    with torch.no_grad():\n",
    "        bt_preds = pred_model(data)\n",
    "    pred_arr.extend(list(bt_preds.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6117314 , 0.9379415 , 0.9002631 , ..., 0.6145652 , 0.63429594,\n",
       "       0.8844879 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalized_preds = NORMALIZER.denorm(torch.Tensor(pred_arr)).numpy()\n",
    "finalized_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6141cf0efbfd4bd9ab2c2f7e</td>\n",
       "      <td>0.611731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6141cf0fe689ecc4c43cdd4b</td>\n",
       "      <td>0.937941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6141cf10b842c2e72e2f2d44</td>\n",
       "      <td>0.900263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6141cf10b842c2e72e2f2d46</td>\n",
       "      <td>0.614716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6141cf1302d926221cabc549</td>\n",
       "      <td>0.614632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  predictions\n",
       "0  6141cf0efbfd4bd9ab2c2f7e     0.611731\n",
       "1  6141cf0fe689ecc4c43cdd4b     0.937941\n",
       "2  6141cf10b842c2e72e2f2d44     0.900263\n",
       "3  6141cf10b842c2e72e2f2d46     0.614716\n",
       "4  6141cf1302d926221cabc549     0.614632"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({'id': private_dataset['_id'].values, 'predictions': finalized_preds})\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('../data/sample_submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aacd9efd2e917f2085b49ad3eecd2bc8a974d0bb8b89bc48afae7fa44e9f517f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
